{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcDnTipnsgH6"
   },
   "source": [
    "Getting everything set up...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "939GDhTxfy5z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "r1CBHQ5ZqUpl",
    "outputId": "1efc4857-780e-4b20-dc87-1ec887e0b426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.2)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=753a9f317dd123dc459811fa2416fdba2ac22fc71d836952bb308e9b625bc9bb\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-0.0.12\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huPhIQqLJyxm"
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UQ2W-jxYNRwI",
    "outputId": "40ca9651-497b-4c28-e1ea-ad1251d1f860"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BtvlF2i1mXQr"
   },
   "outputs": [],
   "source": [
    "from typing import List, Any, Dict, Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qph6gpGtskGJ"
   },
   "source": [
    "Using my helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fCUGErxgwP6"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from typing import List, Any, Dict, Generator\n",
    "\n",
    "\n",
    "def parse_file(filename: str) -> (List[List[str]], List[List[str]]):\n",
    "    \"\"\"\n",
    "    Function for parsing a file in the CONLL format.\n",
    "\n",
    "    :param filename: path to text file. needs to be in CONLL format.\n",
    "    :return: list of lists of tokens, list of lists of tags.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    tags = []\n",
    "\n",
    "    with open(filename) as f:\n",
    "        sentence = []\n",
    "        sentence_tags = []\n",
    "        for line in f:\n",
    "            line_split = line.split()\n",
    "            if len(line_split) == 0:\n",
    "                if len(sentence) > 1:\n",
    "                  tokens.append(sentence)\n",
    "                  tags.append(sentence_tags)\n",
    "                  sentence = []\n",
    "                  sentence_tags = []\n",
    "            else:\n",
    "                if line_split[0] != \"-DOCSTART-\":\n",
    "                    sentence.append(line_split[0])\n",
    "                    sentence_tags.append(line_split[-1])\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    return tokens[1:], tags[1:]\n",
    "\n",
    "\n",
    "def flatten(list_of_lists: List[List[Any]], unique: bool = False) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Flattens and can reduce a list of lists to unique values\n",
    "\n",
    "    :param list_of_lists: list of lists to flatten\n",
    "    :param unique: specifying whether to return a unique or non-unique list\n",
    "    :return: either a unique or non-unique list\n",
    "    \"\"\"\n",
    "    flattened = [item for sublist in list_of_lists for item in sublist]\n",
    "    if unique:\n",
    "        return list(set(flattened))\n",
    "    else:\n",
    "        return flattened\n",
    "\n",
    "\n",
    "def index_dicts(list_to_index: List[List[Any]], special_tokens: List = None) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Creates the index of tokens or tags to be used in model\n",
    "\n",
    "    :param list_to_index: list to index\n",
    "    :param special_tokens: special tokens, if any, for index\n",
    "    :return: dictionary with indices\n",
    "    \"\"\"\n",
    "    word2idx = {}\n",
    "    flat_list = flatten(list_to_index, unique=True)\n",
    "    if special_tokens:\n",
    "        flat_list = [i for i in flat_list if i not in special_tokens]\n",
    "        flat_list = special_tokens + flat_list\n",
    "\n",
    "    for v, k in enumerate(flat_list):\n",
    "        if k not in word2idx.keys():\n",
    "          word2idx[k] = v\n",
    "\n",
    "    return word2idx\n",
    "\n",
    "\n",
    "def prepare_sentence(sentence: List[str], index: Dict[str, int]):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sentence:\n",
    "    :param index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    idxs = [index[w] if w in index.keys() else index[\"<UNK>\"] for w in sentence]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def convert_to_index(sequences: [List[List[Any]]], index: Dict[str, int]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "\n",
    "    :param sequences:\n",
    "    :param index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    converted = []\n",
    "    for sentence in sequences:\n",
    "        converted.append(prepare_sentence(sentence, index))\n",
    "    return converted\n",
    "\n",
    "\n",
    "def generate_batches(batch_size: int, tokens: List[List[int]], tags: List[List[int]]) -> (Generator, Generator):\n",
    "    \"\"\"\n",
    "\n",
    "    :param batch_size:\n",
    "    :param tokens:\n",
    "    :param tags:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(0, len(tokens), batch_size):\n",
    "        token_chunk = tokens[i:i + batch_size]\n",
    "        tag_chunk = tags[i:i + batch_size]\n",
    "\n",
    "        tokens_padded = pad_sequence([word for word in token_chunk], batch_first=True)\n",
    "        tags_padded = pad_sequence([tag for tag in tag_chunk], batch_first=True)\n",
    "        yield tokens_padded, tags_padded\n",
    "\n",
    "\n",
    "def word_to_index(word):\n",
    "    return word2idx_train[word]\n",
    "\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def sentence_to_tensor(sentence, n_labels, unpadded=False):\n",
    "    tensor = torch.zeros(len(sentence), dtype=torch.long)\n",
    "    if unpadded:\n",
    "        #sentence should be a list of words\n",
    "        for num, word in enumerate(sentence):\n",
    "            if word not in word2idx_train.keys():\n",
    "                tensor[num] = word2idx_train[\"<UNK>\"]\n",
    "            else:\n",
    "                tensor[num] = word2idx_train[word]\n",
    "    else:\n",
    "        for num, word in enumerate(sentence):\n",
    "            tensor[num] = word\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def generate_batches_by_len(tokens, tags):\n",
    "    tokens_len = [(len(s), s) for s in tokens]\n",
    "    tags_len = [(len(t), t) for t in tags]\n",
    "\n",
    "    min_len = tokens_len[0][0]\n",
    "    max_len = tokens_len[-1][0]\n",
    "\n",
    "    for i in range(min_len, max_len + 1):\n",
    "        tokens_to_return = [s[1] for s in tokens_len if s[0] == i]\n",
    "        tags_to_return = [s[1] for s in tags_len if s[0] == i]\n",
    "\n",
    "        tokens_tensor = torch.zeros([len(tokens_to_return), i], dtype=torch.long)\n",
    "        tags_tensor = torch.zeros([len(tags_to_return), i], dtype=torch.long)\n",
    "\n",
    "        for s in range(len(tokens_to_return)):\n",
    "            tokens_tensor[s] = tokens_to_return[s]\n",
    "            tags_tensor[s] = tags_to_return[s]\n",
    "\n",
    "        yield tokens_tensor, tags_tensor\n",
    "\n",
    "\n",
    "def predict(X, model, index):\n",
    "    predictions = []\n",
    "    for line in X:\n",
    "        output = model(line)\n",
    "        output_arg_max = [torch.argmax(i).item() for i in output]\n",
    "        sentence = [index[tag] for tag in output_arg_max]\n",
    "        predictions.append(sentence)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ejAfNR8aqYs-",
    "outputId": "4f28111d-996c-4c60-ce94-8ca2fc94e195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3edI0WSsr2R"
   },
   "source": [
    "Read in and format data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Qcl3TUQqP-y"
   },
   "outputs": [],
   "source": [
    "train_tokens, train_tags = parse_file(\"train.txt\")\n",
    "valid_tokens, valid_tags = parse_file(\"valid.txt\")\n",
    "test_tokens, test_tags = parse_file(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQ_jYMg_kf5d"
   },
   "outputs": [],
   "source": [
    "train_tokens_sorted = sorted(train_tokens, key=len)\n",
    "train_tags_sorted = sorted(train_tags, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhWiUeqJmf4C"
   },
   "outputs": [],
   "source": [
    "special_tokens = [\"<PAD>\", \"<UNK>\"]\n",
    "word2idx_train = index_dicts(train_tokens, special_tokens=special_tokens)\n",
    "tag2idx_train = index_dicts(train_tags, special_tokens=[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "h77os7P_O3sp",
    "outputId": "fd5220db-16e8-4fa4-aa9e-93aef4645fd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " 'B-LOC': 8,\n",
       " 'B-MISC': 3,\n",
       " 'B-ORG': 6,\n",
       " 'B-PER': 7,\n",
       " 'I-LOC': 1,\n",
       " 'I-MISC': 4,\n",
       " 'I-ORG': 2,\n",
       " 'I-PER': 5,\n",
       " 'O': 9}"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IS0sEFgsfkbV"
   },
   "outputs": [],
   "source": [
    "train_tokens_indexed = convert_to_index(train_tokens_sorted, word2idx_train)\n",
    "train_tags_indexed = convert_to_index(train_tags_sorted, tag2idx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3QIyejf-sw6a"
   },
   "source": [
    "Create the model. I decided to use a bi-directional LSTM as it is a standard choice for this sort of entity recognition. My model has an embedding layer, the recurrent neural network layer, and a linear hidden state layer. It's fairly simple and doesn't have a ton of parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruVlE7JM_1UX"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 tag_size, \n",
    "                 embedding_dim, \n",
    "                 n_hidden, \n",
    "                 batch_size, \n",
    "                 dropout_p):\n",
    "        \"\"\"\n",
    "\n",
    "        :param vocab_size:\n",
    "        :param embedding_dim:\n",
    "        :param n_hidden:\n",
    "        \"\"\"\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                      embedding_dim=embedding_dim) #,\n",
    "                                      #padding_idx=pad_idx)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=embedding_dim,\n",
    "                           hidden_size=n_hidden,\n",
    "                           num_layers=2,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=True)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.n_labels = tag_size\n",
    "        self.hidden2label = nn.Linear(n_hidden * 2, tag_size)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "        # first is the hidden h\n",
    "        # second is the cell c\n",
    "        return (Variable(torch.zeros(2, self.batch_size, self.n_hidden)),\n",
    "                Variable(torch.zeros(2, self.batch_size, self.n_hidden)))\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.dropout(self.embedding(sentence))\n",
    "        output, (hidden, cell) = self.rnn(embeds.view(len(sentence), 1, -1))\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        return self.hidden2label(hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8BWONiFtGGH"
   },
   "source": [
    "Here I'm setting up the parameters. I played around with these a little to get my current output. To calculate loss, I'm using cross entropy loss, and ignoring index 0 to calculate the loss, because index 0 is the PAD token I use for padding batches to the same length. For optimization I'm using Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9po2FHACs9DR"
   },
   "outputs": [],
   "source": [
    "labels = torch.tensor([i for i in tag2idx_train.values()], dtype=torch.long)\n",
    "\n",
    "batch_size = 32\n",
    "vocab_size = len(word2idx_train)\n",
    "tag_size = len(labels)\n",
    "embedding_dim = 128\n",
    "n_hidden = 256\n",
    "dropout = 0.5\n",
    "#pad_idx = 0\n",
    "learning_rate = 0.001\n",
    "\n",
    "bilstm = BidirectionalLSTM(vocab_size, tag_size, embedding_dim, n_hidden, batch_size, dropout)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "optimizer_bi = optim.Adam(bilstm.parameters(), lr=learning_rate)\n",
    "\n",
    "idx2tag_train = {}\n",
    "for k, v in tag2idx_train.items():\n",
    "    idx2tag_train[v] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUrTXBv1tkzo"
   },
   "source": [
    "Here is my training loop. I didn't try to train it for a high number of epochs. You can see the epoch output below. If I had had more time, I would want to really try to optimize for my F1 score. I don't necessarily care about accuracy here, since 83% of my tags are O. What I want is a good balance of precision and recall on the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "ozLIoNyos9GS",
    "outputId": "205c9a72-c79a-4c13-a27f-57cb0dccc7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch 1 Loss: 0.6271941661834717\n",
      "Epoch 1 F1: 0.00573532261189692\n",
      "Epoch 1 Acc: 0.8238232289497466\n",
      "Epoch 2\n",
      "Epoch 2 Loss: 0.5823267102241516\n",
      "Epoch 2 F1: 0.01026282030196375\n",
      "Epoch 2 Acc: 0.805130446740796\n",
      "Epoch 3\n",
      "Epoch 3 Loss: 0.5258107781410217\n",
      "Epoch 3 F1: 0.014123163131110986\n",
      "Epoch 3 Acc: 0.7860496640603513\n",
      "Epoch 4\n",
      "Epoch 4 Loss: 0.4870966970920563\n",
      "Epoch 4 F1: 0.016008760494342182\n",
      "Epoch 4 Acc: 0.7719834191190916\n",
      "Epoch 5\n",
      "Epoch 5 Loss: 0.4852295219898224\n",
      "Epoch 5 F1: 0.015240443896424169\n",
      "Epoch 5 Acc: 0.7625633570390161\n",
      "Epoch 6\n",
      "Epoch 6 Loss: 0.44016608595848083\n",
      "Epoch 6 F1: 0.016926440510614288\n",
      "Epoch 6 Acc: 0.7538505363247023\n",
      "Epoch 7\n",
      "Epoch 7 Loss: 0.4210341274738312\n",
      "Epoch 7 F1: 0.017009502642142744\n",
      "Epoch 7 Acc: 0.7465374641467919\n"
     ]
    }
   ],
   "source": [
    "bilstm.train()\n",
    "for epoch in range(7):\n",
    "    print(\"Epoch\", epoch + 1)\n",
    "    running_loss = 0.0\n",
    "    batch_index = 0\n",
    "\n",
    "    n = 0\n",
    "    for batch_x, batch_y in generate_batches(batch_size, train_tokens_indexed, train_tags_indexed):\n",
    "\n",
    "        # collect outputs for loss calc\n",
    "        out_tensor = torch.zeros([batch_x.shape[0], batch_x.shape[-1], tag_size])\n",
    "\n",
    "        optimizer_bi.zero_grad()\n",
    "        for i in range(len(batch_x)):\n",
    "            output = bilstm(batch_x[i])\n",
    "            \n",
    "            out_tensor[i] = output\n",
    "            output_arg_max = [torch.argmax(i).item() for i in output]\n",
    "\n",
    "        loss = criterion(out_tensor.permute(0, 2, 1), batch_y)\n",
    "\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_bi.step()\n",
    "        n += 1\n",
    "    \n",
    "    y_pred =  predict(train_tokens_indexed, bilstm, idx2tag_train)\n",
    "    print(f\"Epoch {epoch + 1} Loss:\", running_loss)\n",
    "    print(f\"Epoch {epoch + 1} F1:\", f1_score(train_tags, y_pred))\n",
    "    print(f\"Epoch {epoch + 1} Acc:\", accuracy_score(train_tags, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYjjMBtst8BA"
   },
   "source": [
    "Now that training is done, let's evaluate on the validation set. The model performs pretty well! The best-looking tag is \"LOC\", which is great since this \"project\" is for a travel broker. PER is the worst performing tag, which intuitively makes sense, since names are extremely varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "b8wgSbaLQzKr",
    "outputId": "d0056b39-ac6a-4592-ec13-2b9bfd783b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5068839561674628\n",
      "Accuracy: 0.9096149726371551\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.81      0.68      0.74      1837\n",
      "      PER       0.27      0.18      0.21      1842\n",
      "      ORG       0.52      0.42      0.47      1339\n",
      "     MISC       0.65      0.60      0.62       922\n",
      "\n",
      "micro avg       0.57      0.46      0.51      5940\n",
      "macro avg       0.55      0.46      0.50      5940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_tokens_indexed = convert_to_index(valid_tokens, word2idx_train)\n",
    "valid_tags_indexed = convert_to_index(valid_tags, tag2idx_train)\n",
    "\n",
    "bilstm.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = predict(valid_tokens_indexed, bilstm, idx2tag_train)\n",
    "\n",
    "print(\"F1 score:\", f1_score(valid_tags, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(valid_tags, y_pred))\n",
    "print(classification_report(valid_tags, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9fALsziuVCA"
   },
   "source": [
    "At this point I decided to save my model for use in the Flask app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZ7vK2SZ8ucQ"
   },
   "outputs": [],
   "source": [
    "torch.save(bilstm.state_dict(), \"bilstm.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63SgEgyJuaaZ"
   },
   "source": [
    "Some helper functions for writing and reading the token and tag indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSBgcXCMBTx2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def dict_to_file(index: Dict[Any, Any], filepath: str):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(index, f)\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "def file_to_dict(filepath: str, int_key: bool = False) -> Dict[Any, Any]:\n",
    "    \"\"\"\n",
    "    Convert json file into dictionary.\n",
    "\n",
    "    :param filepath: Name of file to be converted.\n",
    "    :param int_key: If the dict keys are integers, return them as such.\n",
    "    :return: Dictionary!\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        if int_key is True:\n",
    "            loaded = json.load(f)\n",
    "            return {int(key): val for key, val in loaded.items()}\n",
    "        else:\n",
    "            return json.load(f)\n",
    "\n",
    "def load_model(path, token_index, tag_index):\n",
    "    \"\"\"\n",
    "\n",
    "    :param path:\n",
    "    :param token_index:\n",
    "    :param tag_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    batch_size = 32\n",
    "    vocab_size = len(token_index)\n",
    "    tag_size = len(tag_index)\n",
    "    embedding_dim = 128\n",
    "    n_hidden = 256\n",
    "    dropout = 0.5\n",
    "\n",
    "    model = BidirectionalLSTM(vocab_size, tag_size, embedding_dim, n_hidden, batch_size, dropout)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "1cIKD0jtLW8G",
    "outputId": "84118864-8605-4011-e67d-35934e261801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "dict_to_file(word2idx_train, \"train_index.json\")\n",
    "dict_to_file(idx2tag_train, \"tag_index.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPTIrj29vcEM"
   },
   "source": [
    "Let's load it back in and make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ev1Xb9PBjdTu"
   },
   "outputs": [],
   "source": [
    "word2idx_load = file_to_dict(\"train_index.json\")\n",
    "idx2tag_load = file_to_dict(\"tag_index.json\", int_key=True)\n",
    "\n",
    "reloaded_model = load_model(\"bilstm.pt\", word2idx_load, idx2tag_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AVqVvOhuwBMm"
   },
   "source": [
    "Everything works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "wct2wBPujAF2",
    "outputId": "e9677b88-6f81-4c09-db0a-73b35a3c2d65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5068839561674628\n",
      "Accuracy: 0.9096149726371551\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.81      0.68      0.74      1837\n",
      "      ORG       0.52      0.42      0.47      1339\n",
      "      PER       0.27      0.18      0.21      1842\n",
      "     MISC       0.65      0.60      0.62       922\n",
      "\n",
      "micro avg       0.57      0.46      0.51      5940\n",
      "macro avg       0.55      0.46      0.50      5940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reloaded_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_load = predict(valid_tokens_indexed, reloaded_model, idx2tag_load)\n",
    "\n",
    "print(\"F1 score:\", f1_score(valid_tags, y_pred_load))\n",
    "print(\"Accuracy:\", accuracy_score(valid_tags, y_pred_load))\n",
    "print(classification_report(valid_tags, y_pred_load))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxU-_ZLZwLJv"
   },
   "source": [
    "At this point, you would want to incorporate the validation tokens into the training set. I haven't done that here for the sake of time, but that would give the model more information and more data to train over.\n",
    "\n",
    "Finally, let's test it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8S6N5ZKFa-Ju"
   },
   "outputs": [],
   "source": [
    "test_tokens_indexed = convert_to_index(test_tokens, word2idx_load)\n",
    "test_tags_indexed = convert_to_index(test_tags, tag2idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "g7Vabcavxc6l",
    "outputId": "a9873dc4-c106-41fb-b64a-1cacad754d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.45670610407287193\n",
      "Accuracy: 0.8919623992065887\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      ORG       0.49      0.34      0.40      1660\n",
      "      LOC       0.78      0.67      0.72      1666\n",
      "      PER       0.19      0.08      0.12      1615\n",
      "     MISC       0.56      0.52      0.54       701\n",
      "\n",
      "micro avg       0.56      0.39      0.46      5642\n",
      "macro avg       0.50      0.39      0.43      5642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reloaded_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_test = predict(test_tokens_indexed, reloaded_model, idx2tag_load)\n",
    "\n",
    "print(\"F1 score:\", f1_score(test_tags, y_pred_test))\n",
    "print(\"Accuracy:\", accuracy_score(test_tags, y_pred_test))\n",
    "print(classification_report(test_tags, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JnT4PRKfw4wi"
   },
   "source": [
    "Not bad. Location is still the best tag overall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0th42wdYwuBn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
