<!DOCTYPE html>
<html lang="en">
<body>
{% filter markdown %}
Named Entity Recognition
=============

I had a lot of fun with this take home! Please see the sections below for information about each phase and how to run the corresponding code.

### API
To send a sentence to the API for prediction, open another tab in your terminal and run the following code:
`curl -i -X POST -d '{"sentence":"England Reuters"}'  http://localhost:8000/api/predict -H 'Content-Type: application/json'`

The output should look something like:
```
{
  "prediction:": [
    [
      "England",
      "B-LOC"
    ],
    [
      "Reuters",
      "B-ORG"
    ]
  ]
}
```

Feel free to replace `England Reuters` in the curl request with whatever sentence you want!

### Directory structure
* **data**: data for modeling.
* **model**
    * bilstm.pt: trained and saved bi-directional LSTM model.
    * model.py: modeling code.
    * tag_index.json: saved tag index.
    * train.py: script for training the model. Note - I trained my model on Colab.
    * train_index.json: saved token dictionary.
* **notebooks**
    * exploratory.ipynb: exploratory notebook.
    * lstm.ipynb: notebook for building the model on Colab.
* **templates**: pages for this small web app.
* **utils**
    * app_utils.py: utility functions for running the app.
    * data_process.py: data processing utility functions.
* Dockerfile
* main.py
* api.py
* requirements.txt

### Phase 1 - Exploratory and data processing
* [Exploratory notebook]({{url_for('exploratory')}})
* [Data processing documentation]({{url_for('data_processing')}}) Note - I autogenerated the documentation from my docstrings and there aren't newlines between the params, which makes it hard to read. Either way, if you click on "show source" you can see the formatted strings.

### Phase 2 - Modeling
* [Model notebook]({{url_for('model_notebook')}}) In the notebook, you can see my process for building the LSTM, as well as the training, validation, and test evaluation output.
* [Model documentation]({{url_for('model_code')}})

### Phase 3 - Deployment
Both the web app and the api very simply deployed using Flask. I decided to document everything in the web app, since I already had to create an endpoint for prediction. Getting a prediction from the API endpoint only requires using `curl` to send a post request. The sentence input needs to work for input formatted like `'{"sentence":"England Reuters"}'`, otherwise it won't work.

### Resources used
* Libraries
    * PyTorch
    * Pandas
    * Flask
    * pdoc
    * numpy
    * seqeval
* Web resources
    * Stackoverflow, of course
    * PyTorch forums (discuss.pytorch.org)
    * Google Colab

### Possible improvements
* What would I like to try next?
    * Well, there are a few unfinished things here:
        * On the training data, my F1 score is incredibly bad. I would need more time to identify why, but my intuition is that the incredibly large number of O tags is making precision/recall on the training set very poor.
        * Incorporate validation data into training data and retrain
        * I wanted to experiment with other loss functions and optimizers but I didn't have time.
        * Something more focused on travel data would be good for the client.
    * Conditional random fields can be incredibly useful for identifying meaningful transitions in sequences. I would have loved to have implemented one here.
    * I'd also like to experiment with multiple layers.
    * The web app and API are very primitive. It would be great to create a suite of endpoints for the API, so I could retrain, evaluate on a file/different kinds of input, etc. It would also be great to have, for example, a text box where I could type in the sentence and get back the predictions, or have a variety of more sophisticated input mechanisms.
* Once we gather our own annotated data, could we reuse this model or would we create a new one?
    * If the annotations are in the same scheme, then yes! We could add in our own annotated data and retrain. If we're primarily interested in people and what they're doing in various locations, we might consider building that into the model as a weight.
* We plan to expand our offering to multiple different languages next year. Could we use a single model for multiple languages? Is there a way we could leverage our existing model for that?
    * We can definitely use a bi-directional LSTM for sequence prediction, but a seq2seq model that leverages attention for language translation would be better. This particular LSTM likely wouldn't be able to "remember" enough in order to perform translation properly.



{% endfilter %}
</body>
</html>